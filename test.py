import re
import requests
from datetime import datetime
import sqlite3
from bs4 import BeautifulSoup
import concurrent.futures
import asyncio
import httpx

class DLADataImporter:
    def __init__(self, database_file='cve_data.db', url="https://salsa.debian.org/security-tracker-team/security-tracker/-/raw/master/data/DLA/list"):
        """
        Initialize the DLADataImporter class.

        Args:
            url (str): The URL of the data source.
            database_file (str): Path to the SQLite database file.
        """
        self.url = url
        self.database_file = database_file

    def get_data_from_url(self):
        """
        Retrieve the data from the specified URL.

        Returns:
            str: The retrieved data.
        """
        try:
            response = requests.get(self.url)
            response.raise_for_status()
        except requests.exceptions.RequestException as e:
            print("Error occurred during the request:", str(e))
            return ""

        return response.text

    def parse_dla_data(self, data):
        """
        Parse the DLA data.

        Args:
            data (str): The DLA data to parse.

        Returns:
            list: A list of dictionaries containing the parsed DLA data.
        """
        pattern = r"\[(.*?)\]\s(.*?)\s-\ssecurity\supdate\n\s*\{([^}]+)\}\n\s*\[(.*?)\]\s-\s(.*?)\s(.*?)\s"
        matches = re.findall(pattern, data, re.DOTALL)
        parsed_data = []
        for match in matches:
            date_str = match[0]
            dla_id = match[1].split(" ")[0]
            package_name = match[1].split(" ")[1]
            os_version = match[3]
            fixed_package = match[4] + " " + match[5]
            cve_list = match[2].split(" ")
            cve_list = ', '.join(cve_list)
            
            try:
                date = datetime.strptime(date_str, "%d %b %Y").strftime("%Y-%m-%d")
            except ValueError:
                print("Error occurred while parsing date:", date_str)
                continue
            
            # make a resource_Url, that will you to crawl description
            year = datetime.strptime(date, "%Y-%m-%d").year
            if dla_id.endswith("-1"):
                # If DLA ID have format dla-3193-1, the url will look like https://www.debian.org/lts/security/2022/dla-3193 
                url = f"https://www.debian.org/lts/security/{year}/{dla_id[:-2].lower()}"
            else:
                # else url look like https://www.debian.org/lts/security/2023/dla-3193-2 ; keep same dla_id in url
                url = f"https://www.debian.org/lts/security/{year}/{dla_id.lower()}"
            

            parsed_data.append({
                "date": str(date),
                "dla_id": dla_id,
                "package_name": package_name,
                "cve_list": cve_list,
                "os_version": os_version,
                "fixed_package": fixed_package,
                "severity": None,
                "description": None,
                "resource_url": url,
                "mitigation_action": None
            })

        return parsed_data

    def save_data_to_database(self, parsed_data, cursor):
        """
        Save the parsed DLA data to an SQLite database.

        Args:
            parsed_data (list): The parsed DLA data to save.
        """
        

        # Create the DLA Data table if it doesn't exist
        cursor.execute(
            """
            CREATE TABLE IF NOT EXISTS dla_data (
                dla_id TEXT NOT NULL PRIMARY KEY,
                released_on DATE,
                severity TEXT,
                description TEXT,
                package_name TEXT,
                cves TEXT,
                os_version TEXT,
                fixed_package TEXT,
                resource_url TEXT,
                mitigation_action TEXT
            )
            """
        )
        cursor.connection.commit()

        # Insert parsed data into the database
        # Extract required fields and form a list of tuples
        data_list = [(data['dla_id'], data['date'], data['severity'], data['description'],
                    data['package_name'], data['cve_list'], data['os_version'],
                    data['fixed_package'], data['resource_url'], data['mitigation_action']) for data in parsed_data]

        # Insert the data
        try:
            cursor.executemany('INSERT INTO dla_data (dla_id, released_on, severity, description, package_name, cves, os_version, fixed_package, resource_url, mitigation_action) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)', data_list)
        except Exception as error:
            print("Error inserting bulk DLA data into database. Details: " + str(error))

        cursor.connection.commit()

    def get_all_resource_urls(self, cursor):
        """
        Get all resource url from the database where the description is NULL.

        Args:
            cursor (str): cursor SQLite database.

        Returns:
            list: A list of resource URLs.
        """
        cursor.execute("SELECT dla_id, resource_url  FROM dla_data WHERE description is NULL")
        rows = cursor.fetchall()
        # Create a list comprehension to create the JSON objects
        json_data = [{"dla_id": row[0], "resource_url": row[1]} for row in rows]
        # Return the JSON array
        return json_data


    async def get_dla_desc_from_url(self, url):
        """
        Crawl the issue overview content from the specified URL.

        Args:
            url (str): The URL of the HTML page.

        Returns:
            description: description of DLA
            mitigation_action: Mitigation action of DLA.
        """
        description= ""
        mitigation_action = ""
        async with httpx.AsyncClient(timeout=30) as client:
            try:
                response = await client.get(url)
                response.raise_for_status()
            
                html = response.content
                # Parse the HTML content
                soup = BeautifulSoup(html, 'html.parser')
                # Find the relevant <dd> tag
                dd_tag = soup.find('dt', string='More information:').find_next('dd')
                # Extract the first <p> block within <dd> and store it in 'description' variable
                description = re.sub(r'\s+', ' ', dd_tag.find('p').get_text(strip=True))
                # Extract all remaining <p> blocks within <dd> and store them in 'mitigation_action' variable
                p_tags = dd_tag.find_all('p')
                mitigation_action = ' '.join(re.sub(r'\s+', ' ', p.get_text(strip=True)) for p in p_tags[1:])
                
            except httpx.RequestError as exc:
                print(f"An error occurred while requesting {exc.request.url!r}.")
            except httpx.HTTPStatusError as exc:
                print(f"Error response {exc.response.status_code} while requesting {exc.request.url!r}.")
        return description, mitigation_action
        
    
    async def crawl_dla_description(self, rs):
        """
        Retrieve the DLA description from the URL and save it to the database.

        Args:
            rs (json): contain dla_id, resource_url
        """
        print(rs['resource_url'])
        description, mitigation_action = await self.get_dla_desc_from_url(rs['resource_url'])
        if description and mitigation_action:
            self.save_dla_desc_to_database(description, mitigation_action, rs['dla_id'].upper())

    def save_dla_desc_to_database(self, description, mitigation_action, dla_id):
        """
        Save the DLA description to the database.

        Args:
            description (str): The ALAS description.
            mitigation_action (str): The DLA mitigation action.
            dla_id (str): The DLA ID.
        """
        conn = sqlite3.connect(self.database_file, check_same_thread=False)
        cursor = conn.cursor()
        if description:
            try:
                cursor.execute("SELECT dla_id FROM dla_data WHERE dla_id=?", (dla_id,))
                existing_entry = cursor.fetchone()
                if existing_entry:
                    # Update the 'description' field for the existing entry
                    cursor.execute("UPDATE dla_data SET description=?, mitigation_action=? WHERE dla_id=?", (description, mitigation_action, dla_id))
                    print("Update description for DLA ID: " + dla_id)
                else:
                    print("Did not exist DLA ID: " + dla_id)
            except Exception as error:
                # handle the exception
                print("An exception occurred:", error)

        cursor.connection.commit()
        conn.close()

    async def run_crawlers(self):
        conn = sqlite3.connect(self.database_file, check_same_thread=False)
        cursor = conn.cursor()
        rss = self.get_all_resource_urls(cursor)
        conn.close()

        tasks = [self.crawl_dla_description(rs) for rs in rss]
        await asyncio.gather(*tasks)
        
    def main(self):
        """
        The main entry point of the program.
        """
        conn = sqlite3.connect(self.database_file, check_same_thread=False)
        cursor = conn.cursor()

        data = self.get_data_from_url()
        parsed_data = self.parse_dla_data(data)
        self.save_data_to_database(parsed_data, cursor)
        print("Finish to get DLA info from URL")

        loop = asyncio.get_event_loop()
        loop.run_until_complete(self.run_crawlers())

        conn.close()



dla = DLADataImporter(database_file="cve_data.db")
dla.main()