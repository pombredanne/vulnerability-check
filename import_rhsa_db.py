import requests
import sqlite3
import datetime
import concurrent.futures

def get_data_from_url(url):
    response = requests.get(url)
    if response.status_code == 200:
        return response.json()
    else:
        return None

def process_url(url, cursor):
    data = get_data_from_url(url)
    if data is not None:
        # Process and save the data to the database
        save_rhsa_desc_to_database(data, cursor)

def get_rhsa_description(url):
    response = get_data_from_url(url)
    #print(response)
    return response['cvrfdoc']['vulnerability'][0]['notes']['note']

def create_table(cursor):
    cursor.execute("""
        CREATE TABLE IF NOT EXISTS rhsa_data (
            rhsa_id TEXT PRIMARY KEY,
            severity TEXT,
            description,
            CVEs TEXT,
            released_on TEXT,
            resource_url TEXT
        )
    """)

def save_rhsa_to_database(data, cursor):

    for entry in data:
        rhsa_id = entry['RHSA']
        severity = entry['severity']
        CVEs = ', '.join(entry['CVEs'])
        released_on = entry['released_on']
        resource_url = entry['resource_url']
        
        # Check if rhsa_id already exists in the database
        cursor.execute("SELECT rhsa_id FROM rhsa_data WHERE rhsa_id=?", (rhsa_id,))
        existing_entry = cursor.fetchone()
        
        if existing_entry:
            print("Skipping existing data for: " + rhsa_id)
        else:
            cursor.execute("""
                INSERT INTO rhsa_data (rhsa_id, severity, CVEs, released_on, resource_url)
                VALUES (?, ?, ?, ?, ?)
            """, (rhsa_id, severity, CVEs, released_on, resource_url))
            print("Saved data for: " + rhsa_id)

    cursor.connection.commit()

# def save_rhsa_desc_to_database(data, cursor):


def process_rsha_data(cursor):
    current_year = datetime.datetime.now().year
    max_end_year = current_year + 1
    for start_year in range(2000, max_end_year):
        end_year = start_year + 1
        url = f"https://access.redhat.com/hydra/rest/securitydata/cvrf.json?per_page=20000&before={end_year}-01-01&after={start_year}-01-01"
        data = get_data_from_url(url)

        if data is not None and len(data) > 0:
            save_rhsa_to_database(data, cursor)
            print("Data saved to database successfully.")
        else:
            print("Failed to retrieve data from the URL.")

def get_all_resource_urls(cursor):
    cursor.execute("SELECT resource_url FROM rhsa_data")
    rows = cursor.fetchall()
    resource_urls = [row[0] for row in rows]
    return resource_urls


def main():
    
    conn = sqlite3.connect('cve_data.db')
    cursor = conn.cursor()
    create_table(cursor)
    # Import rhsa into db following API https://access.redhat.com/documentation/en-us/red_hat_security_data_api/1.0/html/red_hat_security_data_api/cvrf#parameters
    process_rsha_data(cursor)
    
    # Update the description for all RHSA base on resource_url like: https://access.redhat.com/hydra/rest/securitydata/cvrf/RHSA-2023:1403.json
    resource_urls = get_all_resource_urls(cursor)

    MAX_WORKERS = 10  # Set the maximum number of concurrent workers

    with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        futures = [executor.submit(process_url, url, cursor) for url in resource_urls]

        # Wait for the tasks to complete
        concurrent.futures.wait(futures)

    conn.close()
if __name__ == "__main__":
    main()
