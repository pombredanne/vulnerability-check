import requests
import sqlite3
import xml.etree.ElementTree as ET
from datetime import datetime
import concurrent.futures
from bs4 import BeautifulSoup


class ALASDataImporter:
    def __init__(self):
        self.db_file = 'cve_data.db'

    def process_alas_data(self, url, cursor):
        """
        Retrieve XML data from the specified URL and process it,
        storing the extracted data into a SQLite database.

        Args:
            url (str): The URL of the RSS feed.
            cursor (str): cursor SQLite database.

        Returns:
            int: The number of new items inserted into the database.
        """

        # Retrieve XML data from the URL
        response = requests.get(url)
        xml_data = response.content

        # Parse the XML data
        root = ET.fromstring(xml_data)

        # Create a table to store the items if it doesn't exist
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS alas_data (
                alas_id TEXT,
                severity TEXT,
                description TEXT,
                package_name TEXT,
                cve TEXT,
                released_on TEXT,
                last_build_date TEXT,
                resource_url TEXT
            )
        ''')
        cursor.connection.commit()
        # Track the number of new items inserted
        new_items_count = 0

        # Process each item in the XML
        for item in root.findall('channel/item'):
            title = item.find('title').text

            cve = item.find('description').text.strip()
            released_on = datetime.strptime(item.find('pubDate').text, '%a, %d %b %Y %H:%M:%S %Z')
            released_on = released_on.strftime('%Y-%m-%dT%H:%M:%SZ')
            last_build_date = datetime.strptime(item.find('lastBuildDate').text, '%a, %d %b %Y %H:%M:%S %Z')
            last_build_date = last_build_date.strftime('%Y-%m-%dT%H:%M:%SZ')
            resource_url = item.find('link').text

            # Extract ALAS ID, severity, and package name from the title
            alas_id, severity, package_name = self.parse_title(title)

            # Check if the item already exists in the database based on the ALAS ID
            cursor.execute('SELECT COUNT(*) FROM alas_data WHERE alas_id = ?', (alas_id,))
            result = cursor.fetchone()
            item_exists = result[0] > 0

            if not item_exists:
                try:
                    # Insert the item into the database
                    cursor.execute('''
                        INSERT INTO alas_data (alas_id, severity, package_name, cve, released_on, last_build_date, resource_url)
                        VALUES (?, ?, ?, ?, ?, ?, ?)
                    ''', (alas_id, severity, package_name, cve, released_on, last_build_date, resource_url))

                    new_items_count += 1
                except Exception as err:
                    print(err)

        # Commit the changes and close the database connection
        cursor.connection.commit()

        return new_items_count

    def save_alas_desc_to_database(self, description, alas_id):
        """
        Save the ALAS description to the database.

        Args:
            description (str): The ALAS description.
            alas_id (str): The ALAS ID.
        """
        conn = sqlite3.connect(self.db_file, check_same_thread=False)
        cursor = conn.cursor()

        if description:
            try:
                cursor.execute("SELECT alas_id FROM alas_data WHERE alas_id=?", (alas_id,))
                existing_entry = cursor.fetchone()

                if existing_entry:
                    # Update the 'description' field for the existing entry
                    cursor.execute("UPDATE alas_data SET description=? WHERE alas_id=?", (description, alas_id))
                    print("Updated description for: " + alas_id)
                else:
                    print("Did not exist ALAS ID: " + alas_id)
            except Exception as error:
                # handle the exception
                print("An exception occurred:", error)

        cursor.connection.commit()
        conn.close()

    def parse_title(self, title):
        """
        Parse the title to extract the ALAS ID, severity, and package name.

        Args:
            title (str): The title string.

        Returns:
            tuple: The ALAS ID, severity, and package name.
        """
        # Split the title into components
        parts = title.split(' (')

        # Extract the ALAS ID
        alas_id = parts[0].strip()

        # Extract the severity
        severity = parts[1].split('):')[0].strip()

        # Extract the package name
        package_name = parts[1].split('): ')[1].strip()

        return alas_id, severity, package_name

    def get_alas_desc_from_url(self, url):
        """
        Crawl the issue overview content from the specified URL.

        Args:
            url (str): The URL of the HTML page.

        Returns:
            str: The issue overview content.
        """
        # Retrieve HTML content from the URL
        response = requests.get(url)
        html = response.content
        # Create a BeautifulSoup object to parse the HTML
        soup = BeautifulSoup(html, 'html.parser')
        # Find the <div> element with id='issue_overview'
        div_issue_overview = soup.find('div', {'id': 'issue_overview'})
        # Find the <b> element within the <div> block
        b_element = div_issue_overview.find('p')
        # Get the content inside the <b> element
        content = b_element.get_text(strip=True, separator=' ').replace('Issue Overview:', '').strip()
        return content

    def get_all_resource_urls(self, cursor):
        """
        Get all resource URLs from the database where the description is NULL.

        Args:
            cursor (str): cursor SQLite database.

        Returns:
            list: A list of resource URLs.
        """
        cursor.execute("SELECT resource_url FROM alas_data WHERE description is NULL")
        rows = cursor.fetchall()
        resource_urls = [row[0] for row in rows]
        return resource_urls

    def alas_description(self, url):
        """
        Retrieve the ALAS description from the URL and save it to the database.

        Args:
            url (str): The URL of the ALAS description.
        """
        description = self.get_alas_desc_from_url(url)
        alas_id = url.split("/")[-1]
        alas_id = alas_id[:len(alas_id) - 5]
        self.save_alas_desc_to_database(description, alas_id)

    def main(self):
        """
        The main entry point of the program.
        """
        conn = sqlite3.connect(self.db_file, check_same_thread=False)
        cursor = conn.cursor()

        urls = [
            'https://alas.aws.amazon.com/alas.rss',
            'https://alas.aws.amazon.com/AL2/alas.rss',
            'https://alas.aws.amazon.com/AL2023/alas.rss'
        ]
        for url in urls:
            new_items_count = self.process_alas_data(url, cursor)
            print(f'{new_items_count} new items inserted into the database.')

        resource_urls = self.get_all_resource_urls(cursor)

        MAX_WORKERS= 10  # Set the maximum number of concurrent workers

        with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
            futures = [executor.submit(self.alas_description, url) for url in resource_urls]

            # Wait for the tasks to complete
            concurrent.futures.wait(futures)

        conn.close()


if __name__ == "__main__":
    importer = ALASDataImporter()
    importer.main()
