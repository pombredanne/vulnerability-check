import requests
import zipfile
import csv
import re
import sqlite3
import os
from urllib.parse import urlparse


class CWEDataImporter:
    def __init__(self):
        self.urls = [
            'https://cwe.mitre.org/data/csv/699.csv.zip',
            'https://cwe.mitre.org/data/csv/1194.csv.zip',
            'https://cwe.mitre.org/data/csv/919.csv.zip',
            'https://cwe.mitre.org/data/csv/1000.csv.zip'
        ]
        self.db_name = 'cve_data.db'

    def download_cwe_file(self, url):
        """
        Downloads the file from the provided URL and saves it locally.
        """
        try:
            response = requests.get(url)
            response.raise_for_status()
        except requests.exceptions.RequestException as e:
            print(f"Error downloading file: {e}")
            return None

        # Extract the filename from the URL
        parsed_url = urlparse(url)
        filename = os.path.basename(parsed_url.path)

        with open(filename, 'wb') as file:
            file.write(response.content)

        return filename

    def unzip_file(self, zip_file_path):
        """
        Unzips the downloaded file.
        """
        try:
            with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:
                zip_ref.extractall()
        except zipfile.BadZipFile:
            print("Error unzipping file.")
            return False

        return True

    def parse_cwe_csv(self, csv_file_path):
        """
        Parses the CSV file, extracts desired values, and applies regex pattern.
        Returns the result array.
        """
        results = []
        try:
            with open(csv_file_path, 'r', newline='', encoding='utf-8') as csv_file:
                reader = csv.DictReader(csv_file)
                for row in reader:
                    cwe_id = 'CWE-' + row['CWE-ID']
                    common_consequences = row['Common Consequences']
                    matches = re.findall(r'(?<=IMPACT:)([^:]+)', common_consequences)
                    impact = list(dict.fromkeys(matches))
                    result = {'cwe_id': cwe_id.upper(), 'impact': impact}
                    results.append(result)
        except IOError:
            print("Error reading CSV file.")
            return []

        return results

    def remove_files(self, *files):
        """
        Removes the specified files.
        """
        for file in files:
            if os.path.exists(file):
                os.remove(file)

    def main(self):
        """
        Main method to execute the crawler.
        """
        all_results = []
        for url in self.urls:
            # Download the file
            zip_file = self.download_cwe_file(url)
            if not zip_file:
                continue

            # Unzip the file
            if not self.unzip_file(zip_file):
                continue

            # Extract the CSV file name
            csv_file = os.path.splitext(zip_file)[0]

            # Parse CSV and get results
            results = self.parse_cwe_csv(csv_file)
            if not results:
                continue

            # Merge results with all_results
            all_results.extend(results)

            # Remove the downloaded and extracted files
            self.remove_files(zip_file, csv_file)

        # Deduplicate the results
        unique_results = []
        for result in all_results:
            if result not in unique_results:
                unique_results.append(result)

        # Save unique results to SQLite database
        conn = sqlite3.connect(self.db_name)
        cursor = conn.cursor()

        # Drop table if it exists
        cursor.execute("DROP TABLE IF EXISTS cwe_data")
        # Create table if it doesn't exist
        cursor.execute('''CREATE TABLE IF NOT EXISTS cwe_data
                     (cwe_id TEXT PRIMARY KEY, impact TEXT)''')

        # Insert data into the table
        for result in unique_results:
            cursor.execute("INSERT INTO cwe_data VALUES (?, ?)", (result['cwe_id'], ', '.join(result['impact'])))

        conn.commit()
        conn.close()

        print("CWE data crawling completed!")
