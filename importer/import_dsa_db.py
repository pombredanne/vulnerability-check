import re
import requests
from datetime import datetime
import sqlite3
from bs4 import BeautifulSoup
import concurrent.futures
import asyncio
import httpx

class DSADataImporter:
    def __init__(self, database_file='cve_data.db', url="https://salsa.debian.org/security-tracker-team/security-tracker/-/raw/master/data/DSA/list"):
        """
        Initialize the DSADataImporter class.

        Args:
            url (str): The URL of the data source.
            database_file (str): Path to the SQLite database file.
        """
        self.url = url
        self.database_file = database_file

    def get_data_from_url(self):
        """
        Retrieve the data from the specified URL.

        Returns:
            str: The retrieved data.
        """
        
        try:
            response = requests.get(self.url)
            response.raise_for_status()
        except requests.exceptions.RequestException as e:
            print("Error occurred during the request:", str(e))
            return ""

        return response.text

    def parse_dsa_data(self, data):
        """
        Parse the DSA data.

        Args:
            data (str): The DSA data to parse.

        Returns:
            list: A list of dictionaries containing the parsed DSA data.
        """
        DSA_LIST = []

        """Matched event
        [07 Jul 2023] DSA-5450-1 firefox-esr - security update
            {CVE-2023-37201 CVE-2023-37202 CVE-2023-37207 CVE-2023-37208 CVE-2023-37211}
            [bullseye] - firefox-esr 102.13.0esr-1~deb11u1
            [bookworm] - firefox-esr 102.13.0esr-1~deb12u1
        [17 Mar 2023] DSA-5375-1 thunderbird - security update
            {CVE-2023-25751 CVE-2023-25752 CVE-2023-28162 CVE-2023-28164 CVE-2023-28176}
            [bullseye] - thunderbird 1:102.9.0-1~deb11u1
        """
        pattern_1 = re.compile(r"\[(.*?)\] (.*?) - (?:security|regression) update\s*\{([^}]+)\}\n\s*\[(.*?)\] - (.*?) (\S+)", re.MULTILINE)
        matches_1 = pattern_1.findall(data)
        DSA_LIST.extend(matches_1)

        """Matched event

        [17 Mar 2023] DSA-5356-2 sox - regression update
            [bullseye] - sox 14.4.2+git20190427-2+deb11u2
        """
        pattern_2 = re.compile(r"\[(.*?)\] (.*?) - (?:security|regression) update\s+\[(.*?)\] - (.*?) (\S+)", re.MULTILINE)
        matches_2 = pattern_2.findall(data)

        for mt in matches_2:
            tmp_dsa = mt[:2] + ('',) + mt[2:]
            DSA_LIST.append(tmp_dsa)

        parsed_data = []
        for dsa in DSA_LIST:
            date_str = dsa[0]
            dsa_id = dsa[1].split(" ")[0]
            package_name = dsa[1].split(" ")[1]
            os_version = dsa[3]
            fixed_package = dsa[4] + " " + dsa[5]
            cve_list = dsa[2].split(" ")
            cve_list = ', '.join(cve_list)
            
            try:
                date = datetime.strptime(date_str, "%d %b %Y").strftime("%Y-%m-%d")
            except ValueError:
                print("Error occurred while parsing date:", date_str)
                continue
            
            # make a resource_Url, that will you to crawl description
            year = datetime.strptime(date, "%Y-%m-%d").year
            if dsa_id.endswith("-1"):
                # If DSA ID have format dsa-5442-1, the url will look like https://www.debian.org/security/2023/dsa-5442
                url = f"https://www.debian.org/security/{year}/{dsa_id[:-2].lower()}"
            else:
                # else url look like https://www.debian.org/security/2023/dsa-5356-2 ; keep same dsa_id in url
                url = f"https://www.debian.org/security/{year}/{dsa_id.lower()}"
            

            parsed_data.append({
                "date": str(date),
                "dsa_id": dsa_id,
                "package_name": package_name,
                "cve_list": cve_list,
                "os_version": os_version,
                "fixed_package": fixed_package,
                "severity": None,
                "description": None,
                "resource_url": url,
                "mitigation_action": None
            })

        return parsed_data

    def save_data_to_database(self, parsed_data, cursor):
        """
        Save the parsed DSA data to an SQLite database.

        Args:
            parsed_data (list): The parsed DSA data to save.
        """
        print("DB creation")

        # Create the DSA Data table if it doesn't exist
        cursor.execute(
            """
            CREATE TABLE IF NOT EXISTS dsa_data (
                dsa_id TEXT NOT NULL PRIMARY KEY,
                released_on DATE,
                severity TEXT,
                description TEXT,
                package_name TEXT,
                cves TEXT,
                os_version TEXT,
                fixed_package TEXT,
                resource_url TEXT,
                mitigation_action TEXT
            )
            """
        )
        cursor.connection.commit()

        # Insert parsed data into the database
        # Extract required fields and form a list of tuples
        data_list = [(data['dsa_id'], data['date'], data['severity'], data['description'],
                    data['package_name'], data['cve_list'], data['os_version'],
                    data['fixed_package'], data['resource_url'], data['mitigation_action']) for data in parsed_data]

        # Insert the data
        try:
            cursor.executemany('INSERT INTO dsa_data (dsa_id, released_on, severity, description, package_name, cves, os_version, fixed_package, resource_url, mitigation_action) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)', data_list)
        except Exception as error:
            print("Error inserting bulk DSA data into database. Details: " + str(error))

        cursor.connection.commit()

    def get_all_resource_urls(self, cursor):
        """
        Get all resource url from the database where the description is NULL.

        Args:
            cursor (str): cursor SQLite database.

        Returns:
            list: A list of resource URLs.
        """
        cursor.execute("SELECT dsa_id, resource_url  FROM dsa_data WHERE description is NULL")
        rows = cursor.fetchall()
        # Create a list comprehension to create the JSON objects
        json_data = [{"dsa_id": row[0], "resource_url": row[1]} for row in rows]
        # Return the JSON array
        return json_data


    async def get_dsa_desc_from_url(self, url):
        """
        Crawl the issue overview content from the specified URL.

        Args:
            url (str): The URL of the HTML page.

        Returns:
            description: description of DSA
            mitigation_action: Mitigation action of DSA.
        """
        description= ""
        mitigation_action = ""
        async with httpx.AsyncClient(timeout=60) as client:
            try:
                response = await client.get(url)
                response.raise_for_status()
            
                html = response.content
                # Parse the HTML content
                soup = BeautifulSoup(html, 'html.parser')
                # Find the relevant <dd> tag
                dd_tag = soup.find('dt', string='More information:').find_next('dd')
                # Extract the first <p> block within <dd> and store it in 'description' variable
                description = re.sub(r'\s+', ' ', dd_tag.find('p').get_text(strip=True))
                # Extract all remaining <p> blocks within <dd> and store them in 'mitigation_action' variable
                p_tags = dd_tag.find_all('p')
                mitigation_action = ' '.join(re.sub(r'\s+', ' ', p.get_text(strip=True)) for p in p_tags[1:])
                
            except httpx.RequestError as exc:
                print(f"An error occurred while requesting {exc.request.url!r}.")
            except httpx.HTTPStatusError as exc:
                print(f"Error response {exc.response.status_code} while requesting {exc.request.url!r}.")
        return description, mitigation_action
        
    
    async def crawl_dsa_description(self, rs):
        """
        Retrieve the DSA description from the URL and save it to the database.

        Args:
            rs (json): contain dsa_id, resource_url
        """
        description, mitigation_action = await self.get_dsa_desc_from_url(rs['resource_url'])
        if description and mitigation_action:
            self.save_dsa_desc_to_database(description, mitigation_action, rs['dsa_id'].upper())

    def save_dsa_desc_to_database(self, description, mitigation_action, dsa_id):
        """
        Save the DSA description to the database.

        Args:
            description (str): The ALAS description.
            mitigation_action (str): The DSA mitigation action.
            dsa_id (str): The DSA ID.
        """
        conn = sqlite3.connect(self.database_file, check_same_thread=False)
        cursor = conn.cursor()
        if description:
            try:
                cursor.execute("SELECT dsa_id FROM dsa_data WHERE dsa_id=?", (dsa_id,))
                existing_entry = cursor.fetchone()
                if existing_entry:
                    # Update the 'description' field for the existing entry
                    cursor.execute("UPDATE dsa_data SET description=?, mitigation_action=? WHERE dsa_id=?", (description, mitigation_action, dsa_id))
                    print("Update description for DSA ID: " + dsa_id)
                else:
                    print("Did not exist DSA ID: " + dsa_id)
            except Exception as error:
                # handle the exception
                print("An exception occurred:", error)

        cursor.connection.commit()
        conn.close()

    async def run_crawlers(self):
        conn = sqlite3.connect(self.database_file, check_same_thread=False)
        cursor = conn.cursor()
        rss = self.get_all_resource_urls(cursor)
        conn.close()

        tasks = [self.crawl_dsa_description(rs) for rs in rss]
        await asyncio.gather(*tasks)
    
    def retry_get_description(self):
        """
        The func to retry get get description of DSA (for DSA that have description is NULL only)
        """
        loop = asyncio.get_event_loop()
        loop.run_until_complete(self.run_crawlers())

    def main(self):
        """
        The main entry point of the program.
        """
        conn = sqlite3.connect(self.database_file, check_same_thread=False)
        cursor = conn.cursor()

        data = self.get_data_from_url()

        parsed_data = self.parse_dsa_data(data)

        self.save_data_to_database(parsed_data, cursor)
        print("Finish to get DSA info from URL")

        loop = asyncio.get_event_loop()
        loop.run_until_complete(self.run_crawlers())

        ## Write another function to retry get descript for all DSA have description = NULL because some failed when get description like this "An error occurred while requesting URL('https://www.debian.org/security/2017/dsa-4000')."

        conn.close()



dsa = DSADataImporter(database_file="cve_data.db")
dsa.retry_get_description()