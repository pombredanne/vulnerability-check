import re
import requests
from datetime import datetime
import sqlite3

class DLADataImporter:
    def __init__(self, database_file='cve_data.db', url="https://salsa.debian.org/security-tracker-team/security-tracker/-/raw/master/data/DLA/list"):
        """
        Initialize the DLADataImporter class.

        Args:
            url (str): The URL of the data source.
            database_file (str): Path to the SQLite database file.
        """
        self.url = url
        self.database_file = database_file
        self.conn = None
        self.cursor = None

    def get_data_from_url(self):
        """
        Retrieve the data from the specified URL.

        Returns:
            str: The retrieved data.
        """
        try:
            response = requests.get(self.url)
            response.raise_for_status()
        except requests.exceptions.RequestException as e:
            print("Error occurred during the request:", str(e))
            return ""

        return response.text

    def parse_dla_data(self, data):
        """
        Parse the DLA data.

        Args:
            data (str): The DLA data to parse.

        Returns:
            list: A list of dictionaries containing the parsed DLA data.
        """
        pattern = r"\[(.*?)\]\s(.*?)\s-\ssecurity\supdate\n\s*\{([^}]+)\}\n\s*\[(.*?)\]\s-\s(.*?)\s(.*?)\s"
        matches = re.findall(pattern, data, re.DOTALL)
        parsed_data = []
        for match in matches:
            date_str = match[0]
            dla_id = match[1].split(" ")[0]
            package_name = match[1].split(" ")[1]
            os_version = match[3]
            fixed_package = match[4] + " " + match[5]
            cve_list = match[2].split(" ")
            cve_list = ', '.join(cve_list)
            
            try:
                date = datetime.strptime(date_str, "%d %b %Y").strftime("%Y-%m-%d")
            except ValueError:
                print("Error occurred while parsing date:", date_str)
                continue
            
            # make a resource_Url, that will you to crawl description
            year = datetime.strptime(date, "%Y-%m-%d").year
            url = f"https://www.debian.org/lts/security/{year}/{dla_id[:-2].lower()}"

            parsed_data.append({
                "date": str(date),
                "dla_id": dla_id,
                "package_name": package_name,
                "cve_list": cve_list,
                "os_version": os_version,
                "fixed_package": fixed_package,
                "severity": None,
                "description": None,
                "resource_url": url
            })

        return parsed_data

    def save_data_to_database(self, parsed_data):
        """
        Save the parsed DLA data to an SQLite database.

        Args:
            parsed_data (list): The parsed DLA data to save.
        """
        self.conn = sqlite3.connect(self.database_file)
        self.cursor = self.conn.cursor()

        # Create the DLA Data table if it doesn't exist
        self.cursor.execute(
            """
            CREATE TABLE IF NOT EXISTS dla_data (
                dla_id TEXT,
                released_on DATE,
                severity TEXT,
                description TEXT,
                package_name TEXT,
                cves TEXT,
                os_version TEXT,
                fixed_package TEXT,
                resource_url TEXT
            )
            """
        )

        # Insert parsed data into the database
        for item in parsed_data:
            self.cursor.execute(
                """
                INSERT INTO dla_data (released_on, dla_id, package_name, cves, os_version, fixed_package, severity, description, resource_url)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)
                """,
                (
                    item["date"],
                    item["dla_id"],
                    item["package_name"],
                    item["cve_list"],
                    item["os_version"],
                    item["fixed_package"],
                    item["severity"],
                    item["description"],
                    item["resource_url"],
                )
            )

        self.conn.commit()
        self.conn.close()

    def main(self):
        """
        Process the DLA data by retrieving, parsing, and saving it to the database.
        """
        data = self.get_data_from_url()
        parsed_data = self.parse_dla_data(data)
        self.save_data_to_database(parsed_data)


dla = DLADataImporter()
dla.main()